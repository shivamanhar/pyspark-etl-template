{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af22dacd-3229-4d92-837e-2fb43c9695b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../../config/config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46553875-f11d-45a7-a718-6678affb67eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run  ../../config/sqlconfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee5c2a5f-ba13-4f3d-af54-926767751977",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, StringType, StructField, StructType, DecimalType\n",
    "from pyspark.sql.functions import col, lit, filter\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f990086f-0b71-4f2e-9526-5899d95926fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class ProductBronzeStreamingETL:\n",
    "    def __init__(self):\n",
    "        self.spark = spark\n",
    "        self.sourch_path = base_path\n",
    "        self.catalog = catalog\n",
    "        self.schema_name = schema_name\n",
    "        self.table_name = bronze_product_tbl       \n",
    "\n",
    "        # Paths\n",
    "        self.schema_path = f\"{schema_path}/_bronze_product_schema\"        \n",
    "        self.checkpoint_path = f\"{checkpoint_path}/_bronze_product_checkpoint\"\n",
    "        self.source_path = f\"{base_path}/product\"      \n",
    "\n",
    "        # Schema\n",
    "        self.schema = self._define_schema()\n",
    "    # ----------------------------\n",
    "    # Define schema\n",
    "    # ----------------------------\n",
    "    def _define_schema(self) -> StructType:\n",
    "        return StructType([\n",
    "            StructField('product_sk', StringType(), True),\n",
    "            StructField('product_key', StringType(), True),\n",
    "            StructField('Product_name', StringType(), True),\n",
    "            StructField('category', StringType(), True),\n",
    "            StructField('brand', StringType(), True),\n",
    "            StructField('price', StringType(), True),\n",
    "            StructField('effective_start_date', StringType(), True),\n",
    "            StructField('effective_end_date', StringType(), True),\n",
    "            StructField('is_current', StringType(), True)\n",
    "        ])\n",
    "\n",
    "    # -----------------------------\n",
    "    # Read streaming data\n",
    "    # -----------------------------\n",
    "    def read_stream(self):\n",
    "        return (\n",
    "            self.spark.readStream\n",
    "            .format(\"cloudFiles\")\n",
    "            .option(\"cloudFiles.format\", \"csv\")\n",
    "            .option(\"cloudFiles.schemaLocation\", '/Volumes/workspace/etl_practice/my_file/_schemas')\n",
    "            .option(\"delimiter\", \",\")\n",
    "            .schema(self.schema)\n",
    "            .load(self.source_path)\n",
    "        )\n",
    "    # -------------------------------\n",
    "    # Data Cleaning\n",
    "    # -------------------------------\n",
    "    def clean_streaming_df(self, df):\n",
    "        \"\"\"\n",
    "        1. Removing exact duplicte rows\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return df.dropDuplicates(['product_sk'])\n",
    "        except Exception as e:\n",
    "            print(f\"Error: clean_streaming_df : {e}\")\n",
    "    \n",
    "    def clean_header_df(self, df, header_col: str):\n",
    "        \"\"\"\n",
    "        1. Removing header row\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return df.filter(col(header_col) !='product_sk')\n",
    "        except Exception as e:\n",
    "            print(f\"Error: clean_header_df : {e}\")\n",
    "\n",
    "    # -----------------------\n",
    "    # Write to delta table (Streaming)\n",
    "    # -----------------------\n",
    "    def write_stream(self, df):\n",
    "        try:\n",
    "            query = (\n",
    "                df.writeStream\n",
    "                .option(\"checkpointLocation\", f\"{self.checkpoint_path}\")\n",
    "                .trigger(availableNow=True)\n",
    "                .toTable(f\"{self.catalog}.{self.schema_name}.{self.table_name}\")\n",
    "            )\n",
    "            query.awaitTermination()\n",
    "        except Exception as e:\n",
    "            print(f\"Error: write_stream : {e}\")\n",
    "\n",
    "    def testing_df(self):\n",
    "        df = self.spark.read.format(\"csv\").load(\"/Volumes/workspace/etl_practice/my_file/product/dim_product.csv\")\n",
    "        return df\n",
    "    def console_test(self, df):\n",
    "        df.writeStream \\\n",
    "        .format(\"console\") \\\n",
    "        .option(\"checkpointLocation\", f\"{self.checkpoint_path}/_debug\") \\\n",
    "        .trigger(availableNow=True) \\\n",
    "        .start() \\\n",
    "        .awaitTermination()\n",
    "    def clean_schema(self):\n",
    "        dbutils.fs.rm(self.checkpoint_path, recurse=True)\n",
    "        dbutils.fs.rm(self.schema_path, recurse=True)\n",
    "        self.spark.sql(f\"drop table {self.catalog}.{self.schema_name}.{self.table_name}\")\n",
    "    \n",
    "    def display_tbl(self):\n",
    "        self.spark.sql(f\"SELECT *FROM {self.catalog}.{self.schema_name}.{self.table_name}\").display()\n",
    "\n",
    "    def run(self):\n",
    "        raw_df = self.read_stream()\n",
    "        cleaned_df = self.clean_streaming_df(raw_df)\n",
    "        cleaned_df = self.clean_header_df(cleaned_df, 'product_sk')\n",
    "        self.write_stream(cleaned_df)\n",
    "        #self.testing_df()\n",
    "        #self.console_test(raw_df)\n",
    "        print(f\"Successful write {self.table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f7e9b6c-73bf-4179-b90d-c70dea17c151",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "obj = ProductBronzeStreamingETL()\n",
    "raw_df = obj.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "597dc951-0856-4f78-92eb-a7f1551a45f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "obj.display_tbl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a0fc8ee-eb62-428a-b6ab-5f19c848938f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#obj.clean_schema()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8380065723593762,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_dim_product_ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
