{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcb735d9-cc65-4039-b155-da1beb62ab2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../../config/config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dac32d9b-d9c0-4526-a85f-cea01ce93d84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../../config/sqlconfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efdb65b8-68d0-422d-997a-34a8d58cfef0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StringType, IntegerType, StructField, DecimalType\n",
    "from pyspark.sql.functions import col, lit,filter\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d41b18c6-21e9-47bd-92a8-0af245253ad0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class SalesStreamingETL:\n",
    "    \"\"\"\n",
    "    Class to handle Sales Streaming ETL using Auto Loader\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_path: str, checkpoint_path: str, schema_path: str):\n",
    "        self.spark = spark\n",
    "        self.base_path = base_path\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        self.schema_path = schema_path\n",
    "\n",
    "        # Paths\n",
    "        self.schema_path = f\"{self.schema_path}/sales_schema\"\n",
    "        self.checkpoint_path = f\"{self.checkpoint_path}/sales_table\"\n",
    "        self.source_path = f\"{base_path}/sales/\"\n",
    "\n",
    "        # Schema\n",
    "        self.schema = self._define_schema()\n",
    "\n",
    "    # -------------------------------\n",
    "    # Define schema\n",
    "    # -------------------------------\n",
    "    def _define_schema(self) -> StructType:\n",
    "        return StructType([\n",
    "            StructField(\"sales_id\", IntegerType(), False),\n",
    "            StructField(\"transaction_ts\", StringType(), True),\n",
    "            StructField(\"date_key\", IntegerType(), True),\n",
    "            StructField(\"product_key\", IntegerType(), True),\n",
    "            StructField(\"customer_key\", IntegerType(), True),\n",
    "            StructField(\"store_key\", IntegerType(), True),\n",
    "            StructField(\"payment_key\", IntegerType(), True),\n",
    "            StructField(\"quantity\", IntegerType(), True),\n",
    "            StructField(\"unit_price\", DecimalType(10, 2), True),\n",
    "            StructField(\"discount_pct\", DecimalType(5, 2), True),\n",
    "            StructField(\"net_sales_amount\", DecimalType(10, 2), True),\n",
    "            StructField(\"tax_amount\", DecimalType(10, 2), True)\n",
    "        ])\n",
    "\n",
    "    # -------------------------------\n",
    "    # Read streaming data\n",
    "    # -------------------------------\n",
    "    def read_stream(self, file_name, delimiter_type: str):\n",
    "        return (\n",
    "            self.spark.readStream\n",
    "            .format(\"cloudFiles\")\n",
    "            .option(\"cloudFiles.format\", \"csv\")\n",
    "            .option(\"cloudFiles.schemaLocation\", f\"{self.schema_path}/{file_name}\")\n",
    "            .option(\"delimiter\", delimiter_type)\n",
    "            .schema(self.schema)\n",
    "            .load(f\"{self.source_path}/{file_name}\")\n",
    "        )\n",
    "\n",
    "    # -------------------------------\n",
    "    # Data Cleaning\n",
    "    # -------------------------------\n",
    "    def clean_streaming_df(self, df):\n",
    "        \"\"\"\n",
    "        Clean streaming DataFrame by:\n",
    "        1. Removing header row if present\n",
    "        2. Removing exact duplicate rows\n",
    "        \"\"\"\n",
    "        try:            \n",
    "            return df.dropDuplicates()\n",
    "        except Exception as e:\n",
    "            print(f\"Error cleaning streaming DataFrame: {e}\")\n",
    "\n",
    "    def clean_header_df(self, df, header_col: str):\n",
    "        \"\"\"\n",
    "        Clean streaming DataFrame by:\n",
    "        1. Removing header row if present\n",
    "        2. Removing exact duplicate rows\n",
    "        \"\"\"\n",
    "        try:            \n",
    "            withoutduplicate =  df.filter(df[header_col] != header_col)\n",
    "            return withoutduplicate\n",
    "        except Exception as e:\n",
    "            print(f\"Error cleaning streaming DataFrame: {e}\")\n",
    "\n",
    "    # -------------------------------\n",
    "    # Write to Delta Table (Streaming)\n",
    "    # -------------------------------\n",
    "    def write_stream(self, df, folder_name):\n",
    "        try:\n",
    "            query =  (\n",
    "            df.writeStream\n",
    "            .option(\"checkpointLocation\", f\"{self.checkpoint_path}/{folder_name}\")\n",
    "            .trigger(availableNow=True)\n",
    "            .toTable(\"workspace.etl_practice.fact_sales\")\n",
    "            )\n",
    "            query.awaitTermination()\n",
    "        except Exception as e:\n",
    "            print(f\"Streaming job failed: {e}\")\n",
    "\n",
    "    # -------------------------------\n",
    "    # Run ETL\n",
    "    # -------------------------------\n",
    "    def run(self, folder_name, delimiter): \n",
    "        raw_df = self.read_stream(folder_name, delimiter)\n",
    "        cleaned_df = self.clean_streaming_df(raw_df)\n",
    "        cleaned_df = self.clean_header_df(cleaned_df, 'transaction_ts')\n",
    "        self.write_stream(cleaned_df, folder_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "355d5544-945d-44da-a486-17b2321a8142",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Config / Parameters\n",
    "# -------------------------------\n",
    "source_path = f\"{base_path}\"\n",
    "checkpoint_path = f\"{checkpoint_path}/sales_table\"\n",
    "schema_path = f\"{base_path}/sales_schema\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ed0e4ad-005e-4e4b-9ed6-608034b153f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "subfolders = dbutils.fs.ls(f\"{source_path}/sales\")\n",
    "for item in subfolders:\n",
    "    delimiter = ''\n",
    "    folder_name = ''\n",
    "    if(item.name == 'comma/'):\n",
    "        delimiter = ','\n",
    "        folder_name ='comma'\n",
    "    if(item.name == 'pipe/'):\n",
    "        delimiter = '|'\n",
    "        folder_name ='pipe'\n",
    "    if(item.name == 'semicolon/'):\n",
    "        delimiter = ';'\n",
    "        folder_name ='semicolon'\n",
    "    obj = SalesStreamingETL(source_path, checkpoint_path, schema_path)\n",
    "    obj.run(folder_name, delimiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "095e04dd-36d7-4ebc-833f-75bbfff9e54a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- select * from fact_sales order by sales_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f61821b1-4441-444b-8efd-069ec791c66a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#.option(\"cloudFiles.partitionColumns\", 'transaction_ts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e8bb6f4-532c-4507-947d-6fb4bc4a703c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- select *from csv.`/Volumes/workspace/etl_practice/my_file/sales/comma/sales_001.csv`  with(header=\"true\", delimiter= \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d22b72fc-b671-4b99-b75d-75380a8a39a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- select *from csv.`/Volumes/workspace/etl_practice/my_file/sales/pipe/sales_003.csv` with(header=\"true\", delimiter= \"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af1303d5-4004-4909-abb2-9af535c6a255",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- select *from csv.`/Volumes/workspace/etl_practice/my_file/sales/semicolon/sales_002.csv` with (   header = \"true\",  delimiter = \";\");"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_fact_sales_ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
