{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcb735d9-cc65-4039-b155-da1beb62ab2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../../config/config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dac32d9b-d9c0-4526-a85f-cea01ce93d84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../../config/sqlconfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efdb65b8-68d0-422d-997a-34a8d58cfef0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StringType, IntegerType, StructField, DecimalType\n",
    "from pyspark.sql.functions import col, lit,filter\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d41b18c6-21e9-47bd-92a8-0af245253ad0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class SalesStreamingETL:\n",
    "    \"\"\"\n",
    "    Class to handle Sales Streaming ETL using Auto Loader\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_path: str, checkpoint_path: str, schema_path: str):\n",
    "        self.spark = spark\n",
    "        self.base_path = base_path\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        self.schema_path = schema_path\n",
    "\n",
    "        # Paths\n",
    "        self.schema_path = f\"{self.schema_path}/customer_schema\"\n",
    "        self.checkpoint_path = f\"{self.checkpoint_path}/customer_table\"\n",
    "        self.source_path = f\"{base_path}/customer/\"\n",
    "\n",
    "        # Schema\n",
    "        self.schema = self._define_schema()\n",
    "\n",
    "    # -------------------------------\n",
    "    # Define schema\n",
    "    # -------------------------------\n",
    "    def _define_schema(self) -> StructType:\n",
    "        return StructType([\n",
    "            StructField(\"customer_key\", StringType(), False),\n",
    "            StructField(\"customer_name\", StringType(), True),\n",
    "            StructField(\"gender\", StringType(), True),\n",
    "            StructField(\"city\", StringType(), True)\n",
    "        ])\n",
    "\n",
    "    # -------------------------------\n",
    "    # Read streaming data\n",
    "    # -------------------------------\n",
    "    def read_stream(self):\n",
    "        return (\n",
    "            self.spark.readStream\n",
    "            .format(\"cloudFiles\")\n",
    "            .option(\"cloudFiles.format\", \"csv\")\n",
    "            .option(\"cloudFiles.schemaLocation\", f\"{self.schema_path}/customer_schema\")\n",
    "            .option(\"delimiter\", ',')\n",
    "            .schema(self.schema)\n",
    "            .load(f\"{self.source_path}\")\n",
    "        )\n",
    "\n",
    "    # -------------------------------\n",
    "    # Data Cleaning\n",
    "    # -------------------------------\n",
    "    def clean_streaming_df(self, df):\n",
    "        \"\"\"\n",
    "        Clean streaming DataFrame by:\n",
    "        1. Removing header row if present\n",
    "        2. Removing exact duplicate rows\n",
    "        \"\"\"\n",
    "        try:            \n",
    "            return df.dropDuplicates()\n",
    "        except Exception as e:\n",
    "            print(f\"Error cleaning streaming DataFrame: {e}\")\n",
    "\n",
    "    def clean_header_df(self, df, header_col: str):\n",
    "        \"\"\"\n",
    "        Clean streaming DataFrame by:\n",
    "        1. Removing header row if present\n",
    "        2. Removing exact duplicate rows\n",
    "        \"\"\"\n",
    "        try:            \n",
    "            withoutduplicate =  df.filter(df[header_col] != header_col)\n",
    "            return withoutduplicate\n",
    "        except Exception as e:\n",
    "            print(f\"Error cleaning streaming DataFrame: {e}\")\n",
    "\n",
    "    # -------------------------------\n",
    "    # Write to Delta Table (Streaming)\n",
    "    # -------------------------------\n",
    "    def write_stream(self, df):\n",
    "        try:\n",
    "            query =  (\n",
    "            df.writeStream\n",
    "            .option(\"checkpointLocation\", f\"{self.checkpoint_path}/bronze_customer\")\n",
    "            .trigger(availableNow=True)\n",
    "            .toTable(\"workspace.etl_practice.bronze_customer\")\n",
    "            )\n",
    "            query.awaitTermination()\n",
    "        except Exception as e:\n",
    "            print(f\"Streaming job failed: {e}\")\n",
    "\n",
    "    # -------------------------------\n",
    "    # Run ETL\n",
    "    # -------------------------------\n",
    "    def run(self): \n",
    "        raw_df = self.read_stream()\n",
    "        cleaned_df = self.clean_streaming_df(raw_df)\n",
    "        cleaned_df = self.clean_header_df(cleaned_df, 'customer_key')\n",
    "        self.write_stream(cleaned_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "355d5544-945d-44da-a486-17b2321a8142",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Config / Parameters\n",
    "# -------------------------------\n",
    "source_path = f\"{base_path}\"\n",
    "checkpoint_path = f\"{checkpoint_path}/customer_table\"\n",
    "schema_path = f\"{base_path}/customer_schema\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ed0e4ad-005e-4e4b-9ed6-608034b153f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "obj = SalesStreamingETL(source_path, checkpoint_path, schema_path)\n",
    "obj.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "095e04dd-36d7-4ebc-833f-75bbfff9e54a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from bronze_customer order by customer_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7e7c00a-0ee2-4b88-a2c6-8ace6e59a297",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bronze_customer_df = spark.table(\"workspace.etl_practice.bronze_customer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "751c2ab0-e574-44c8-9cad-ca068c5ee9f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bronze_customer_df.display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3617598792775171,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_dim_customer_ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
